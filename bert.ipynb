{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "bfc76d5d5b9aca0883f92617b6d72c13a78df34bd63a2979eae8ef60118fbc90"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "import bert_tokenizer as tokenizer\n",
    "from bert import tokenization\n",
    "#from bert import bert_tokenization\n",
    "from bert.tokenization import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#import tensorflow as tf\n",
    "# !pip install tensorflow==1.12.0\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "import bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data processing\"\n",
    "cols = [\"sentiment\",\"id\",\"date\",\"query\",\"user\",\"text\"]\n",
    "data = pd.read_csv('training.1600000.processed.noemoticon.csv', header=None,\n",
    "names=cols,\n",
    "encoding=\"latin1\")\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"id\",\"date\",\"query\",\"user\"],\n",
    "axis=1,\n",
    "inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING\n",
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet,\"lxml\").get_text()\n",
    "    # tweet = BeautifulSoup(tweet,'lxml')\n",
    "    # tweet = tweet.get_text()\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ',tweet)\n",
    "    tweet = re.sub(r\"https?://A-Za-z0-9]+\", ' ',tweet)\n",
    "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ',tweet)\n",
    "    tweet = re.sub(r\" +\", ' ',tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET CREATION\n",
    "\n",
    "#We will create padded batches (so we pad sentences for each batch independetly),this way we add the minimum of padding tokens possible.For that,we sort sentences by lenghth ,apply padded_batches and then shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = [clean_tweet(tweet) for tweet in data.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = data.sentiment.values\n",
    "data_labels[data_labels == 4] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "source": [
    "# fullTokenizer = bert.tokenization.FullTokenizer()\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\",trainable=False)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "#FullTokenizer = bert.tokenization.FullTokenizer(vocab_file)\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file,do_lower_case)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sent):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_len = [[sent, data_labels[i],len (sent)]\n",
    "                for i, sent in enumerate(data_inputs)]\n",
    "\n",
    "random.shuffle(data_with_len)\n",
    "data_with_len.sort(key=lambda x:x[2])\n",
    "sorted_all = [(sent_lab[0],sent_lab[1])\n",
    "for sent_lab in data_with_len if sent_lab[2] > 7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = tf.data.Dataset.from_generator(lambda:sorted_all,output_types=(tf.int32,tf.int32))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "all_batched = all_dataset.padded_batch(BATCH_SIZE,padded_shapes=((None, ),()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_BATCHES = math.ceil(len(sorted_all)/BATCH_SIZE)\n",
    "NB_BATCHES_TEST = NB_BATCHES//10\n",
    "all_batched.shuffle(NB_BATCHES)\n",
    "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
    "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNN(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "    vocab_size,\n",
    "    emb_dim=128,\n",
    "    nb_filters=50,\n",
    "    FFN_units=512,\n",
    "    nb_classes=2,\n",
    "    dropout_rate=0.1,\n",
    "    training= False,\n",
    "    name = \"dcnn\"):\n",
    "\n",
    "        super(DCNN,self).__init__(name=name)\n",
    "\n",
    "    self.embedding = layers.Embedding(vocab_size,emb_dim)\n",
    "    self.bigram = layers.Convolution1D(filters=nb_filters,\n",
    "                  kernel_size=2,\n",
    "                  padding=\"valid\",\n",
    "                  activation=\"relu\")\n",
    "    self.trigram = layers.Convolution1D(filters=nb_filters,\n",
    "                  kernel_size=3,\n",
    "                  padding=\"valid\",\n",
    "                  activation=\"relu\")\n",
    "    self.fourgram = layers.Convolution1D(filters=nb_filters,\n",
    "                  kernel_size=3,\n",
    "                  padding=\"valid\",\n",
    "                  activation=\"relu\")\n",
    "\n",
    "    self.pool = layers.GlobalAvgPool1D()\n",
    "\n",
    "    self.dense_1 = layers.Dense(units=FFN_units,activation=\"relu\")\n",
    "\n",
    "    self.dropout - layers.Dropout(rate= dropout_rate)\n",
    "\n",
    "    if nb_classes ==2:\n",
    "        self.last_dense = layers.Dense(units=1,activation=\"sigmoid\")\n",
    "    else:\n",
    "        self.last_dense = layers.Dense(units=nb_classes,activation=\"softmax\")\n",
    "\n",
    "\n",
    "def call(self,inputs,training):\n",
    "    x= self.embedding(inputs)\n",
    "    x_1 = self.bigram(x)\n",
    "    x_1 = self.pool(x_1)\n",
    "    x_2= self.trigram(x)\n",
    "    x_2 = self.pool(x_2)\n",
    "    x_3 = self.fourgram(x)\n",
    "    x_3 = self.pool(x_3)  #(batch_size,nb_filters)\n",
    "\n",
    "    merged = tf.concat([x_1,x_2,x_3],axis=-1) #batch_size, 3*nb_filters\n",
    "    merged = self.dense_1(merged)\n",
    "    merged = self.dropout(merged,training)\n",
    "    output = self.last_dense(merged)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " VOCAB_SIZE = len(tokenizer.vocab)\n",
    "EMB_DM = 200\n",
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = 2\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "source": [
    "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
    "        emb_dim=EMB_DM,\n",
    "        nb_filters=NB_FILTERS,\n",
    "        nb_classes=NB_CLASSES,\n",
    "        dropout_rate=DROPOUT_RATE)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NB_CLASSES ==2:\n",
    "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
    "    optimizer = \"admin\",\n",
    "    metrics= [\"accuracy\"])\n",
    "\n",
    "else:\n",
    "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer = \"adam\",\n",
    "    metrics = [\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"C:\\Users\\F5390087\\Downloads\\Udemy Bert\\ckpt_bert_tok\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(Denn=Denn)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt,checkpoint_path,max_to_keep=1)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt_manager.restore(ckpt_manager.latest_checkpoit)\n",
    "    print(\"Latest checkpoint restored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        ckpt_manager.save()\n",
    "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dcnn.fit(train_dataset,\n",
    "         epochs=NB_EPOCHS,\n",
    "         callbacks=[MyCustomCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Dcnn.evaluate(test_dataset)\n",
    "print(resuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(sentence):\n",
    "    tokens = encode_sentence(sentence)\n",
    "    inputs = tf.expand_dims(token,0)\n",
    "\n",
    "    output = Dcnn(inouts,training= False)\n",
    "\n",
    "    sentiment = math.floor(output*2)\n",
    "\n",
    "    if sentiment ==0:\n",
    "        print(\"output of the model: {}\\nPredicted sentiment: negative.\".format(output))\n",
    "\n",
    "    elif sentiment ==1:\n",
    "        print(\"output of the model: {}\\nPredicted sentiment: positive.\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction(\"this movie was pretty interesting\")"
   ]
  }
 ]
}