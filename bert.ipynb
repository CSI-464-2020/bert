{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import math\r\n",
    "import re\r\n",
    "import pandas as pd\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import random\r\n",
    "\r\n",
    "import bert_tokenizer as tokenizer\r\n",
    "from bert import tokenization\r\n",
    "#from bert import bert_tokenization\r\n",
    "from bert.tokenization import *\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "#import tensorflow as tf\r\n",
    "# !pip install tensorflow==1.12.0\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow_hub as hub\r\n",
    "from tensorflow.keras import layers\r\n",
    "import bert\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data processing\"\n",
    "cols = [\"sentiment\",\"id\",\"date\",\"query\",\"user\",\"text\"]\n",
    "data = pd.read_csv('training.1600000.processed.noemoticon.csv', header=None,\n",
    "names=cols,\n",
    "encoding=\"latin1\")\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=[\"id\",\"date\",\"query\",\"user\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING\r\n",
    "def clean_tweet(tweet):\r\n",
    "    tweet = BeautifulSoup(tweet,\"lxml\").get_text()\r\n",
    "    # tweet = BeautifulSoup(tweet,'lxml')\r\n",
    "    # tweet = tweet.get_text()\r\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ',tweet)\r\n",
    "    tweet = re.sub(r\"https?://A-Za-z0-9]+\", ' ',tweet)\r\n",
    "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ',tweet)\r\n",
    "    tweet = re.sub(r\" +\", ' ',tweet)\r\n",
    "    return tweet\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET CREATION\r\n",
    "\r\n",
    "#We will create padded batches (so we pad sentences for each batch independetly),this way we add the minimum of padding tokens possible.For that,we sort sentences by lenghth ,apply padded_batches and then shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\F5390087\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \" i just received my G8 viola exam.. and its... well... .. disappointing.. :\\..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\F5390087\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \"E3 ON PLAYSTATION HOME IN ABOUT AN HOUR!!!!!!!!!! \\../  \\../\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_clean = [clean_tweet(tweet) for tweet in data.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = data.sentiment.values\n",
    "data_labels[data_labels == 4] =1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\r\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\",trainable=False)\r\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\r\n",
    "#FullTokenizer = bert.tokenization.FullTokenizer(vocab_file)\r\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\r\n",
    "tokenizer = FullTokenizer(vocab_file,do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sent):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_len = [[sent, data_labels[i],len (sent)]\n",
    "                for i, sent in enumerate(data_inputs)]\n",
    "\n",
    "random.shuffle(data_with_len)\n",
    "data_with_len.sort(key=lambda x:x[2])\n",
    "sorted_all = [(sent_lab[0],sent_lab[1])\n",
    "for sent_lab in data_with_len if sent_lab[2] > 7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = tf.data.Dataset.from_generator(lambda:sorted_all,output_types=(tf.int32,tf.int32))\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "all_batched = all_dataset.padded_batch(BATCH_SIZE,padded_shapes=((None, ),()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_BATCHES = math.ceil(len(sorted_all)/BATCH_SIZE)\n",
    "NB_BATCHES_TEST = NB_BATCHES//10\n",
    "all_batched.shuffle(NB_BATCHES)\n",
    "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
    "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNN(tf.keras.Model):\r\n",
    "    \r\n",
    "    #vocab_size = 20000\r\n",
    "\r\n",
    "\r\n",
    "    def __init__(self,vocab_size,emb_dim=128,nb_filters=50, FFN_units=512, nb_classes=2,dropout_rate=0.1,training= False,name = \"dcnn\"):\r\n",
    "        \r\n",
    "        super(DCNN,self).__init__(name=name)\r\n",
    "\r\n",
    "        self.embedding = layers.Embedding(vocab_size,emb_dim)\r\n",
    "        self.bigram = layers.Convolution1D(filters=nb_filters,\r\n",
    "                  kernel_size=2,\r\n",
    "                  padding=\"valid\",\r\n",
    "                  activation=\"relu\")\r\n",
    "        self.trigram = layers.Convolution1D(filters=nb_filters,\r\n",
    "                  kernel_size=3,\r\n",
    "                  padding=\"valid\",\r\n",
    "                  activation=\"relu\")\r\n",
    "        self.fourgram = layers.Convolution1D(filters=nb_filters,\r\n",
    "                  kernel_size=3,\r\n",
    "                  padding=\"valid\",\r\n",
    "                  activation=\"relu\")\r\n",
    "\r\n",
    "        self.pool = layers.GlobalAvgPool1D()\r\n",
    "\r\n",
    "        self.dense_1 = layers.Dense(units=FFN_units,activation=\"relu\")\r\n",
    "\r\n",
    "        self.dropout = layers.Dropout(rate= dropout_rate)\r\n",
    "\r\n",
    "        if nb_classes ==2:\r\n",
    "\r\n",
    "           self.last_dense = layers.Dense(units=1,activation=\"sigmoid\")\r\n",
    "        else:\r\n",
    "           self.last_dense = layers.Dense(units=nb_classes,activation=\"softmax\")\r\n",
    "\r\n",
    "\r\n",
    "    def call(self,inputs,training):\r\n",
    "        x= self.embedding(inputs)\r\n",
    "        x_1 = self.bigram(x)\r\n",
    "        x_1 = self.pool(x_1)\r\n",
    "        x_2= self.trigram(x)\r\n",
    "        x_2 = self.pool(x_2)\r\n",
    "        x_3 = self.fourgram(x)\r\n",
    "        x_3 = self.pool(x_3)  #(batch_size,nb_filters)\r\n",
    "\r\n",
    "        merged = tf.concat([x_1,x_2,x_3],axis=-1) #batch_size, 3*nb_filters\r\n",
    "        merged = self.dense_1(merged)\r\n",
    "        merged = self.dropout(merged,training)\r\n",
    "        output = self.last_dense(merged)\r\n",
    "\r\n",
    "        return output\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.vocab)\r\n",
    "EMB_DM = 200\r\n",
    "NB_FILTERS = 100\r\n",
    "FFN_UNITS = 256\r\n",
    "NB_CLASSES = 2\r\n",
    "\r\n",
    "DROPOUT_RATE = 0.2\r\n",
    "\r\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
    "        emb_dim=EMB_DM,\n",
    "        nb_filters=NB_FILTERS,\n",
    "        nb_classes=NB_CLASSES,\n",
    "        dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NB_CLASSES ==2:\r\n",
    "    Dcnn.compile(loss=\"binary_crossentropy\",\r\n",
    "    optimizer = \"adam\",\r\n",
    "    metrics= [\"accuracy\"])\r\n",
    "\r\n",
    "else:\r\n",
    "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\r\n",
    "    optimizer = \"adam\",\r\n",
    "    metrics = [\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "checkpoint_path = (r\"C:\\Users\\F5390087\\Downloads\\Udemy Bert\\ckpt_bert_tok\")\r\n",
    "\r\n",
    "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\r\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt,checkpoint_path,max_to_keep=1)\r\n",
    "\r\n",
    "\r\n",
    "if ckpt_manager.latest_checkpoint:\r\n",
    "    print (\"in\")\r\n",
    "    ckpt_manager.restore(ckpt_manager.latest_checkpoint)\r\n",
    "    print(\"Latest checkpoint restored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        ckpt_manager.save()\n",
    "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "37586/37586 [==============================] - 4453s 118ms/step - loss: 0.4288 - accuracy: 0.8013\n",
      "Checkpoint saved at C:\\Users\\F5390087\\Downloads\\Udemy Bert\\ckpt_bert_tok.\n",
      "Epoch 2/5\n",
      "37586/37586 [==============================] - 4094s 108ms/step - loss: 0.3810 - accuracy: 0.8293\n",
      "Checkpoint saved at C:\\Users\\F5390087\\Downloads\\Udemy Bert\\ckpt_bert_tok.\n",
      "Epoch 3/5\n",
      "37586/37586 [==============================] - 3970s 105ms/step - loss: 0.3463 - accuracy: 0.8485\n",
      "Checkpoint saved at C:\\Users\\F5390087\\Downloads\\Udemy Bert\\ckpt_bert_tok.\n",
      "Epoch 4/5\n",
      "37586/37586 [==============================] - 4153s 110ms/step - loss: 0.3122 - accuracy: 0.8664\n",
      "Checkpoint saved at C:\\Users\\F5390087\\Downloads\\Udemy Bert\\ckpt_bert_tok.\n",
      "Epoch 5/5\n",
      "37586/37586 [==============================] - 4665s 123ms/step - loss: 0.2811 - accuracy: 0.8813\n",
      "Checkpoint saved at C:\\Users\\F5390087\\Downloads\\Udemy Bert\\ckpt_bert_tok.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f91e813be0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dcnn.fit(train_dataset,\n",
    "         epochs=NB_EPOCHS,\n",
    "         callbacks=[MyCustomCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4176/4176 [==============================] - 27s 6ms/step - loss: 0.8801 - accuracy: 0.8242\n",
      "[0.8800873160362244, 0.8241663575172424]\n"
     ]
    }
   ],
   "source": [
    "results = Dcnn.evaluate(test_dataset)\r\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(sentence):\r\n",
    "    tokens = encode_sentence(sentence)\r\n",
    "    inputs = tf.expand_dims(tokens,0)\r\n",
    "\r\n",
    "    output = Dcnn(inputs,training= False)\r\n",
    "\r\n",
    "    sentiment = math.floor(output*2)\r\n",
    "\r\n",
    "    if sentiment ==0:\r\n",
    "        print(\"output of the model: {}\\nPredicted sentiment: negative.\".format(output))\r\n",
    "\r\n",
    "    elif sentiment ==1:\r\n",
    "        print(\"output of the model: {}\\nPredicted sentiment: positive.\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(get_prediction(\"this movie was pretty interesting\"))\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUTS \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only use the first sentence for BERT inputs so we add the CLS token at the beginning and the SEP token at the end of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sent):\r\n",
    "    return[\"[CLS]\"] + tokenizer.tokenize(sent)+ [\"[SEP]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs = [encode_sentence(sent) for sent in data_clean]\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET CREATION \r\n",
    "\r\n",
    "We will createvthe 3 different inputs for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(tokens):\r\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\r\n",
    "\r\n",
    "def get_mask(tokens):\r\n",
    "    return np.char.not_equal(tokens,\"[PAD]\").astype(int)\r\n",
    "\r\n",
    "def get_segments(tokens):\r\n",
    "    seg_ids = []\r\n",
    "    current_seg_id=0\r\n",
    "    for tok in tokens:\r\n",
    "        seg_ids.append(current_seg_id)\r\n",
    "        if tok == \"[SEP]\":\r\n",
    "            current_sig_id = 1-current_seg_id\r\n",
    "    return seg_ids\r\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create padded batches (so we pad sentences for each batch independently)this way we add the minimum of possible.For that,we get sentences by length,apply padded_batches and then shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfc76d5d5b9aca0883f92617b6d72c13a78df34bd63a2979eae8ef60118fbc90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}